{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression: interative lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to say is that logistic regression is not a prediction, but a **classification** learning\n",
    "algorithm. The name *logistic regression* comes from statistics and is due to the fact that the mathematical\n",
    "formulation of logistic regression is similar to that of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we still want to learn the equation of the hyperplane which best describes (fits) the data. Only the **target variable is** not numeric but **categorical**. In the simplest case, each data point belongs to one of two complimentary classes: positive (1) and negative (0). \n",
    "\n",
    "The classification problem that we are trying to solve: given a vector of (numeric) observations $\\mathbf{x}$, predict whether this observation belongs to a given class (1) or not (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using simple regression for classification\n",
    "The first idea that comes to mind - to use the usual regression: after all, we can treat class label 0 and 1 as a numeric attribute. Let's see why this idea does not really work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a dataset that contains only one predictive attribute $\\mathbf{x^{(1)}}$ (1D vector), and the categorical target attribute $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a tiny sample dataset\n",
    "Data_X = np.array([1.0, 3.5, 4, 9, 9.5])\n",
    "Data_Y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "X = Data_X.reshape(len(Data_X),1) \n",
    "Y = Data_Y.reshape(len(Data_Y),1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat $y$ as numeric and try a **simple linear regression**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression(normalize=True)\n",
    "\n",
    "regr.fit(X, Y)\n",
    "print('intercept:', regr.intercept_)\n",
    "print('slope:', regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how can we use this line to predict the class: 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data points\n",
    "plt.scatter(X, Y,  c=Y) \n",
    "plt.title('Train Data Fit') \n",
    "plt.xlabel('X') \n",
    "plt.ylabel('Y') \n",
    "\n",
    "# Plot regression line \n",
    "plt.plot(X, regr.predict(X), color = \"red\", linewidth=3) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the above experiment, the best fitting line does not really help us to predict the value of $y$: the linear combination of features such as $\\mathbf{wx} + b$ gives a continuous result\n",
    "that spans from minus infinity to plus infinity, while $y$ has only two possible values.\n",
    "\n",
    "We also see that the $SSR$ would not be a good objective function for fitting this line - no matter how you change the line parameters, the SSR will always remain very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression in 1D\n",
    "The idea of logistic regression is to map an output of a continuous function to a binary result. \n",
    "If we set one class \n",
    "label to 0 and the second class label to 1, we just need to find a simple continuous\n",
    "function whose codomain is (0, 1). In such a case, if the value returned by the model for\n",
    "input $\\mathbf{x}$ is closer to 0, then we output a class label 0, otherwise, the example is labeled\n",
    "as 1.\n",
    "\n",
    "One function that has such a property is the *standard logistic function* (also\n",
    "known as the <b>sigmoid</b> function)\n",
    "<!--<img src=\"images/sigmoid.png\" width=\"400px\">-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(a, b, x):\n",
    "    return a*x + b\n",
    "\n",
    "def sigmoid(y):    \n",
    "    s = 1 / (1 + np.exp(-y)) \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.arange(1., 10., 0.2)\n",
    "a=2\n",
    "b=-10\n",
    "\n",
    "S = sigmoid(line(a,b,x_range))\n",
    "plt.scatter(x_range, S)\n",
    "\n",
    "print(sigmoid(line(a,b,5)))\n",
    "plt.scatter([5],sigmoid(line(a,b,5)), color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take an output of the linear model and feed it into a sigmoid function, then we get a number between 0 and 1. Sigmoid function gives us a probability that the point belongs to class 1.\n",
    "\n",
    "If we determined that $a$ and $b$ parameters of the best line are as above, then all the values of $x$ starting with approximately 6.5 should be classified as class 1 with the probability close to 1.0. All the values of $x$ smaller than 3.5 do not belong to class 1 - their probability is close to 0.\n",
    "\n",
    "For other values of $x$ the probability of belonging or not belonging to class 1 can be found from the above sigmoid function.\n",
    "\n",
    "The value of $x$ which corresponds to the probability 0.5 is called a **decision boundary** - it divides all the points in the dataset into 2 classes - 1 and 0.\n",
    "\n",
    "Try to change the parameters of the line and see how the decision boundary changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Would simple linear regression help?\n",
    "Let's now see how do we use the sigmoid transformation for classification on the same sample toy 1D dataset as in Section 1.\n",
    "\n",
    "Maybe we just need to find the best fitting line and transform it into a sigmoid line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating the dataset here \n",
    "Data_X = np.array([1.0, 3.5, 4, 9, 9.5])\n",
    "Data_Y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "X = Data_X.reshape(len(Data_X),1) \n",
    "Y = Data_Y.reshape(len(Data_Y),1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression(normalize=True)\n",
    "\n",
    "regr.fit(X, Y)\n",
    "\n",
    "a = regr.coef_[0][0]\n",
    "b = regr.intercept_[0]\n",
    "print(\"slope:\",a,\", intercept:\",b)\n",
    "\n",
    "# draw sigmoid predictor\n",
    "#create constinuous interval of x values\n",
    "x_range = np.arange(-5, 15, 0.2)\n",
    "\n",
    "S = sigmoid(line(a,b,x_range))\n",
    "db_point = sigmoid(line(a,b,2.6))\n",
    "\n",
    "plt.plot(x_range, S)\n",
    "plt.scatter(X,Y,c=Y)\n",
    "plt.scatter([2.6],db_point, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the decision boundary? How many points are classified as positive using this decision boundary? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Proper logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the logistic regression learning algorithm is to find the function which best predicts the classes of the data points in the training set. \n",
    "Given (currently unknown) parameters of the line $a$ and $b$, the predicted value $p_i$ for each datapoint looks like this:\n",
    "\n",
    "$p_i = sigmoid(ax_i +b)$\n",
    "\n",
    "Recall that $p_i$ reflects the probability of the point belonging to class 1. Thus (1 - $p_i$) is the probability of the point not belonging to class 1. \n",
    "\n",
    "The objective function should reflect the difference between $p_i$ and the actual class label $y_i$.\n",
    "\n",
    "The probability for each point $(x_i,y_i)$ to be classified correctly:\n",
    "\n",
    "$E(x_i) = p_i^{y_i} * (1-p_i)^{(1-y_i)}$\n",
    "\n",
    "The **objective function of logistic regression** is called *likelihood*. It reflects the probability of observing these data points given the parameters $a$ and $b$. The likelihood of the model given data is just the product of all the probabilities $E(x_i)$:\n",
    "\n",
    "$$L(a,b|data) = \\Pi_{i=1}^n{ E(x_i)}$$\n",
    "\n",
    "The algorithm which finds the best values of $a$ and $b$ is called the **maximum likelihood** computation. It is an iterative algorithm based on gradfient descent! The sigmoid function has a nice derivative (can you calculate it with respect to a and with respect to b?).\n",
    "\n",
    "This time we are looking to maximize the likelihood. So if the derivative of function $L(b)$ is positive for some value of $b$ - we make next step in the same direction. As always we are using the learning rate $\\eta$ to make small moves, and we also set the maximum number of iterations (learning epochs). \n",
    "\n",
    "In practice - because the product of probabilities for multiple points can become really small and cause underflow - we use **log likelihood** and try to maximize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using logistic regression learning algorithm\n",
    "# Fit (train) the Logistic Regression classifier\n",
    "logregr = linear_model.LogisticRegression()\n",
    "\n",
    "# note that while in regular regression Y can be multi-dimensional, \n",
    "# in logistic regression it has to be a 1D vector\n",
    "fitted_model = logregr.fit(X, Data_Y)\n",
    "a = logregr.coef_[0][0]\n",
    "b = logregr.intercept_[0]\n",
    "\n",
    "print(\"slope:\",a,\", intercept:\",b)\n",
    "\n",
    "# draw sigmoid predictor\n",
    "#create constinuous interval of x values\n",
    "x_range = np.arange(-5, 15, 0.2)\n",
    "\n",
    "S = sigmoid(line(a,b,x_range))\n",
    "db_point = sigmoid(line(a,b,6.54))\n",
    "\n",
    "plt.plot(x_range, S)\n",
    "plt.scatter(X,Y,c=Y)\n",
    "plt.scatter(6.54,db_point,c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic regression in 2D\n",
    "\n",
    "For the 2-dimensional vector **x** we are looking for a hyperplane:\n",
    "\n",
    "$y = w_1*x^{(1)} + w_2*x^{(2)}+ w_0$\n",
    "\n",
    "And the predicted value becomes:\n",
    "\n",
    "$p_i = \\frac{1}{1 + e^{-y(x_i^{(1)},x_i^{(2)})}}$\n",
    "\n",
    "Note that as in the case of regression, we do not have to use the linear combination of original features, but may also use polynomials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample input dataset is taken from Conway & Myles Machine Learning for Hackers book, Chapter 2.\n",
    "\n",
    "Each sample contains three columns.\n",
    "* Height in inches\n",
    "* Weight in pounds\n",
    "* Male: 1 corresponds to a male person, and 0 corresponds to a female person.\n",
    "\n",
    "Download the [input dataset](https://docs.google.com/spreadsheets/d/1WXqfBWrgpBbdoyi3p5wHJ0yDrO-NtMlSlf7QjW2jncw/edit?usp=sharing) to your local folder and update the path in the cell below.\n",
    "\n",
    "We want to learn the model which given height and weight would predict a class label: male (1) or female (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_file = \"../../data_ml_2020/height_weight_gender.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = feature values, all the columns except the last column\n",
    "X = data.iloc[:, :-1]\n",
    "# y = target value, class, last column of the data frame\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "# all males\n",
    "males = data.loc[Y == 1]\n",
    "\n",
    "# all females\n",
    "females = data.loc[Y == 0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot\n",
    "plt.scatter(males.iloc[:, 0], males.iloc[:, 1], s=2, label='Males')\n",
    "plt.scatter(females.iloc[:, 0], females.iloc[:, 1], s=2, label='Females')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Fit (train) the Logistic Regression classifier\n",
    "logregr = linear_model.LogisticRegression(C=1e40, solver='newton-cg')\n",
    "fitted_model = logregr.fit(X, Y)\n",
    "\n",
    "# Predict\n",
    "prediction_result = logregr.predict([(70,180)]) #174 cm, 82 kg\n",
    "print(prediction_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"intercept:\", logregr.intercept_ )\n",
    "print( \"slopes:\", logregr.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 70\n",
    "x2 = 180\n",
    "\n",
    "y_predict = logregr.coef_ [0][0]*x1 + logregr.coef_ [0][1]*x2 + logregr.intercept_\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are two features in our dataset, the linear equation can be represented by\n",
    "$y = w_1*x_1 + w_2*x_2 + w_0$\n",
    "\n",
    "or using the parameters learned by logistic regression:\n",
    "$y = -0.49261999*x_1 + 0.19834042*x_2 + 0.69254177$\n",
    "\n",
    "The sigmoid function here is 3-dimensional. The decision boundary for is a projection of the sigmoid into a 2D space ($x_1$, $x_2$).\n",
    "To find the line that separates males from females we set $y$ to zero and compute the line in coordinates ($x_1$, $x_2$):\n",
    "\n",
    "$0 = -0.49261999*x_1 + 0.19834042*x_2 + 0.69254177$\n",
    "\n",
    "$x_2 = \\frac{0.49261999*x_1 - 0.69254177}{0.19834042}$\n",
    "\n",
    "This is the line equation for decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x1 = X.min()[0]\n",
    "print(min_x1)\n",
    "\n",
    "max_x1 = X.max()[0]\n",
    "print(max_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(min_x1 - 5, max_x1 + 5,100)\n",
    "y_values = (0.49261999*x_values - 0.69254177) / 0.19834042\n",
    "plt.plot(x_values, y_values, \"--r\")\n",
    "plt.scatter(males.iloc[:, 0], males.iloc[:, 1], s=2, label='Males')\n",
    "plt.scatter(females.iloc[:, 0], females.iloc[:, 1], s=2, label='Females')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 Marina Barsky. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
